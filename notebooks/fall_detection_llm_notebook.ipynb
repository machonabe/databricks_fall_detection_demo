{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763cd7eb-132c-4723-ab44-a06ce25debe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install opencv-python moviepy mlflow langchain sentence-transformers openai gradio pandas databricks-vectorsearch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474a9efa-0db2-4cdd-a00d-24e61175da7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widgetsの作成\n",
    "dbutils.widgets.text(\"catalog\", \"fall_detection_demo_catalog\", \"カタログ\")\n",
    "dbutils.widgets.text(\"schema\", \"{ご自身のスキーマ名を入力}\", \"スキーマ\") # handson_{名前}\n",
    "dbutils.widgets.text(\"suffix\", \"{ご自身のSuffixを指定}\", \"ENDPOINT用の接尾辞\")\n",
    "dbutils.widgets.dropdown(\"recreate_schema\", \"False\", [\"True\", \"False\"], \"スキーマを再作成\")\n",
    "\n",
    "\n",
    "# Widgetからの値の取得\n",
    "CATALOG = dbutils.widgets.get(\"catalog\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema\")\n",
    "RECREATE_SCHEMA = dbutils.widgets.get(\"recreate_schema\") == \"True\"\n",
    "SUFFIX = dbutils.widgets.get(\"suffix\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,パラメーターのチェック\n",
    "print(f\"catalog: {CATALOG}\")\n",
    "print(f\"schema: {SCHEMA}\")\n",
    "print(f\"recreate_schema: {RECREATE_SCHEMA}\")\n",
    "\n",
    "if not CATALOG:\n",
    "    raise ValueError(\"存在するカタログ名を入力してください\")\n",
    "if not SCHEMA:\n",
    "    raise ValueError(\"スキーマ名を入力してください\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef6bf50-1c6a-408d-a548-8c9ef9cf9c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_schema_sql = f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA};\"\n",
    "spark.sql(create_schema_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953c7475-546a-430c-8dd5-37fdf06da2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_video_volume = f\"\"\"\n",
    "CREATE VOLUME {CATALOG}.{SCHEMA}.video_volume COMMENT '動画ファイルを格納するためのボリューム';\n",
    "\"\"\"\n",
    "spark.sql(create_video_volume)\n",
    "\n",
    "create_frame_volume = f\"\"\"\n",
    "CREATE VOLUME {CATALOG}.{SCHEMA}.frame_volume COMMENT '動画から抽出したフレーム画像を格納するためのボリューム';\n",
    "\"\"\"\n",
    "spark.sql(create_frame_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383decc4-b313-4b1c-825a-7f56f689fe8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Volume パス定義\n",
    "VIDEO_VOL = f\"/Volumes/{CATALOG}/{SCHEMA}/video_volume\"\n",
    "FRAME_VOL = f\"/Volumes/{CATALOG}/{SCHEMA}/frame_volume\"\n",
    "\n",
    "# presigned URL（実際のURLに置き換えてください）\n",
    "presigned_url = 'https://x.gd/Q65MV'\n",
    "\n",
    "volume_path = f\"{VIDEO_VOL}/FallDetection.mp4\"\n",
    "\n",
    "# presigned URLから動画をダウンロードして保存\n",
    "with requests.get(presigned_url, stream=True) as response:\n",
    "    response.raise_for_status()  # ダウンロード失敗時は例外\n",
    "    with open(volume_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "print(f'Downloaded video saved to {volume_path}')\n",
    "\n",
    "def extract_frames_from_video(video_filename, interval_seconds=1):\n",
    "    \"\"\"\n",
    "    Video Volume から読み込み、Frame Volume に画像を出力\n",
    "    \"\"\"\n",
    "    input_path = f\"{VIDEO_VOL}/{video_filename}\"\n",
    "    output_dir = f\"{FRAME_VOL}/{os.path.splitext(video_filename)[0]}_frames\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    skip = int(fps * interval_seconds)\n",
    "    frame_count = 0\n",
    "    saved = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % skip == 0:\n",
    "            ts = frame_count / fps\n",
    "            fname = f\"{output_dir}/frame_{saved:06d}_{ts:.1f}s.jpg\"\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved += 1\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return saved\n",
    "\n",
    "# 実行例\n",
    "num = extract_frames_from_video(\"FallDetection.mp4\", interval_seconds=1)\n",
    "print(f\"抽出したフレーム数: {num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa3aab0-aa7a-49eb-bdc3-0aeac7b10a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import base64\n",
    "import json\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Databricks・MLflow 初期設定\n",
    "# ──────────────────────────────────────────────────────────\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# 実験名は絶対パス＋タイムスタンプで一意化\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "user = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "EXP_PATH = f\"/Users/{user}/{timestamp}_{SUFFIX}\"\n",
    "mlflow.set_experiment(EXP_PATH)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# モデル定義\n",
    "# ──────────────────────────────────────────────────────────\n",
    "MULTIMODAL_MODELS = {\n",
    "    \"databricks-llama-4-maverick\": \"databricks-llama-4-maverick\",\n",
    "    \"databricks-claude-3-7-sonnet\": \"databricks-claude-3-7-sonnet\"\n",
    "}\n",
    "\n",
    "def get_openai_client() -> OpenAI:\n",
    "    \"\"\"Databricks Foundation Model API 用 OpenAI クライアント\"\"\"\n",
    "    return OpenAI(\n",
    "        api_key=dbutils.notebook.entry_point.getDbutils()\n",
    "            .notebook().getContext().apiToken().getOrElse(None),\n",
    "        base_url=f\"https://{spark.conf.get('spark.databricks.workspaceUrl')}/serving-endpoints\"\n",
    "    )\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# 推論プロンプト\n",
    "# ──────────────────────────────────────────────────────────\n",
    "FULL_PROMPT = \"\"\"\n",
    "以下を満たすように画像を分析してください。以下の形式で必ず日本語で答えてください。説明については、自由記述して構いません。：\n",
    "1. 人が転倒しているか\n",
    "2. 危険度\n",
    "3. 緊急対応要否\n",
    "回答形式：\n",
    "転倒検知: はい/いいえ\n",
    "危険度: 高/中/低\n",
    "説明: ...\n",
    "\"\"\"\n",
    "\n",
    "def analyze_image(image_path: str, model_name: str, client: OpenAI):\n",
    "    \"\"\"画像を指定モデルで解析し、結果を返す\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_b64 = base64.b64encode(f.read()).decode()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MULTIMODAL_MODELS[model_name],\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": FULL_PROMPT},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "            ]\n",
    "        }],\n",
    "        max_tokens=400,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    text = resp.choices[0].message.content or \"\"\n",
    "    is_fall = bool(re.search(r\"転倒検知[:：]\\s*(はい|Yes|yes)\", text, re.IGNORECASE))\n",
    "    return text, is_fall\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# Volume 内全画像を評価し、Run 毎に image_path をパラメータとして記録\n",
    "# ──────────────────────────────────────────────────────────\n",
    "def evaluate_volume_images():\n",
    "    if not os.path.exists(FRAME_VOL):\n",
    "        raise FileNotFoundError(f\"Volume パスが存在しません: {FRAME_VOL}\")\n",
    "\n",
    "    img_files = [\n",
    "        os.path.join(r, f)\n",
    "        for r, _, fs in os.walk(FRAME_VOL)\n",
    "        for f in fs if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    if not img_files:\n",
    "        raise RuntimeError(f\"{FRAME_VOL} に画像が見つかりません。\")\n",
    "\n",
    "    client = get_openai_client()\n",
    "    results = []\n",
    "\n",
    "    # 全画像×モデルそれぞれを個別の Run として記録\n",
    "    for image_path in img_files:\n",
    "        image_name = os.path.basename(image_path)\n",
    "        for model in MULTIMODAL_MODELS:\n",
    "            run_name = f\"{image_name}_{model}\"\n",
    "            with mlflow.start_run(run_name=run_name, nested=True):\n",
    "                # Run パラメータに image_path と model を記録\n",
    "                mlflow.log_param(\"image_path\", image_path)\n",
    "                mlflow.log_param(\"model\", model)\n",
    "\n",
    "                # 画像を解析\n",
    "                text, is_fall = analyze_image(image_path, model, client)\n",
    "\n",
    "                # 結果テキストをアーティファクトとして保存\n",
    "                mlflow.log_text(text, \"response.txt\")\n",
    "\n",
    "                # 転倒検知結果をメトリクスとして保存\n",
    "                mlflow.log_metric(\"is_fall\", int(is_fall))\n",
    "\n",
    "                results.append({\n",
    "                    \"image_path\": image_path,\n",
    "                    \"model\": model,\n",
    "                    \"is_fall\": is_fall,\n",
    "                    \"response_artifact\": \"response.txt\"\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Unity Catalog Volume 内の全画像について、モデル別に個別 Run を作成し、\")\n",
    "    print(\"image_path パラメータを表示して MLflow UI で比較可能にします。\")\n",
    "    evaluate_volume_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0dd876e-abc6-4cb4-a3a0-37562b1ffe93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('clip-ViT-B-32')  # 例: CLIPモデル\n",
    "\n",
    "def image_to_vec(img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    return model.encode(image)\n",
    "\n",
    "data = []\n",
    "for root, _, files in os.walk(FRAME_VOL):\n",
    "    for f in files:\n",
    "        if f.endswith(\".jpg\"):\n",
    "            path = os.path.join(root, f)\n",
    "            vec = image_to_vec(path)\n",
    "            data.append({\n",
    "                \"image_path\": path,\n",
    "                \"embedding\": vec.tolist()\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "881c9f8b-96d5-451c-8c52-2339e5f5ef9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"image_path\", T.StringType(), False),\n",
    "    T.StructField(\"embedding\", T.ArrayType(T.FloatType()), False)\n",
    "])\n",
    "\n",
    "spark_df = spark.createDataFrame(df, schema=schema)\n",
    "DELTA_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.frame_embeddings\"\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(DELTA_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31a77d9f-442e-4885-82ab-3f2090f69479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"ALTER TABLE {DELTA_TABLE_NAME} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d7cf75-d1d3-4322-8da8-292a098ae206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "\n",
    "client = VectorSearchClient()\n",
    "endpoint_name = f\"fall_detection_vector_search_{SUFFIX}\"\n",
    "index_name = f\"{CATALOG}.{SCHEMA}.fall_detection_index\"\n",
    "embedding_dimension = 512  # モデルに合わせて変更\n",
    "\n",
    "# 1. エンドポイント作成\n",
    "client.create_endpoint(\n",
    "    name=endpoint_name,\n",
    "    endpoint_type=\"STANDARD\"  # または \"STORAGE_OPTIMIZED\"\n",
    ")\n",
    "\n",
    "\n",
    "# 2. インデックス作成\n",
    "client.create_delta_sync_index(\n",
    "    endpoint_name=endpoint_name,\n",
    "    index_name=index_name,\n",
    "    source_table_name=DELTA_TABLE_NAME,  # Deltaテーブルの完全修飾名\n",
    "    pipeline_type=\"TRIGGERED\",           # または \"CONTINUOUS\"\n",
    "    primary_key=\"image_path\",\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    embedding_vector_column=\"embedding\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13490146-3868-42c2-a8eb-eae8010a2757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "class ImageSearchAgent(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, index_name, embedding_model_name=\"clip-ViT-B-32\"):\n",
    "        self.index_name = index_name\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.model = SentenceTransformer(self.embedding_model_name)\n",
    "        self.vs_client = VectorSearchClient()\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # model_input: dict {\"query\": \"転倒している人の画像\"}\n",
    "        query = model_input[\"query\"]\n",
    "        query_vec = self.model.encode([query])[0]\n",
    "        # ベクトル検索\n",
    "        results = self.vs_client.similarity_search(\n",
    "            index_name=self.index_name,\n",
    "            query_vector=query_vec.tolist(),\n",
    "            k=5  # 上位5件\n",
    "        )\n",
    "        # 結果の画像パスなどを返す\n",
    "        return [r['image_path'] for r in results['matches']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783ceb1f-f4a3-48ef-9bf7-cf9b9b3e9019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.models import ModelSignature\n",
    "from mlflow.models.resources import DatabricksVectorSearchIndex\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "class ImageSearchAgent(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, index_name, embedding_model_name=\"clip-ViT-B-32\"):\n",
    "        self.index_name = index_name\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.model = SentenceTransformer(self.embedding_model_name)\n",
    "        self.vs_client = VectorSearchClient()\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # model_input: pandas.DataFrame with column \"query\"\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            queries = model_input[\"query\"].tolist()\n",
    "        elif isinstance(model_input, dict):\n",
    "            queries = [model_input[\"query\"]]\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a DataFrame or dict with 'query' key.\")\n",
    "        results = []\n",
    "        for query in queries:\n",
    "            query_vec = self.model.encode([query])[0]\n",
    "            # Vector Search API: search\n",
    "            search_result = self.vs_client.search(\n",
    "                index_name=self.index_name,\n",
    "                query_vector=query_vec.tolist(),\n",
    "                top_k=5\n",
    "            )\n",
    "            # 画像パスのみ抽出（適宜修正）\n",
    "            image_paths = [hit['image_path'] for hit in search_result['results'][0]['hits']]\n",
    "            results.append(image_paths)\n",
    "        return results\n",
    "\n",
    "# 入力例と出力例\n",
    "input_example = pd.DataFrame({\"query\": [\"転倒している人の画像\"]})\n",
    "# 仮の出力例（実際はagent.predictで取得）\n",
    "output_example = [[\"/Volumes/twatanabe_demo/fall_catalog_detection/frame_volume/xxx.jpg\"]]\n",
    "\n",
    "# signature\n",
    "signature = infer_signature(input_example, output_example)\n",
    "\n",
    "index_name = \"twatanabe_demo.fall_catalog_detection.fall_detection_index\"\n",
    "registered_model_name = f\"{CATALOG}.{SCHEMA}.image_search_agent\"\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        name=\"image_search_agent\",  # MLflow 3.xではnameを推奨\n",
    "        python_model=ImageSearchAgent(index_name=index_name),\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        resources=[DatabricksVectorSearchIndex(index_name=index_name)]\n",
    "    )\n",
    "    model_uri = f\"runs:/{run.info.run_id}/image_search_agent\"\n",
    "\n",
    "mlflow.register_model(model_uri, registered_model_name)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3532923256595082,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "fall_detection_llm_notebook",
   "widgets": {
    "catalog": {
     "currentValue": "fall_detection_demo_catalog",
     "nuid": "84a8066a-7665-41d7-a453-9ca11293db9f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "fall_detection_demo_catalog",
      "label": "カタログ",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "fall_detection_demo_catalog",
      "label": "カタログ",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "experiment_path": {
     "currentValue": "/Shared/multimodal_fall_detection",
     "nuid": "81cf1d4e-64ef-4b25-a7ad-5539eb686905",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Shared/multimodal_fall_detection",
      "label": "MLflow Experiment Path",
      "name": "experiment_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Shared/multimodal_fall_detection",
      "label": "MLflow Experiment Path",
      "name": "experiment_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "frame_volume_dir": {
     "currentValue": "frame_volume",
     "nuid": "5534b549-cf6f-4987-a0ad-304a22c08b78",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "frame_volume",
      "label": "Frame Volume Dir",
      "name": "frame_volume_dir",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "frame_volume",
      "label": "Frame Volume Dir",
      "name": "frame_volume_dir",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "recreate_schema": {
     "currentValue": "False",
     "nuid": "b568a5f6-d9ee-4b7e-80c5-96a3834401cc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "スキーマを再作成",
      "name": "recreate_schema",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "スキーマを再作成",
      "name": "recreate_schema",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "schema": {
     "currentValue": "handson_twatanabe",
     "nuid": "4f89d91d-63bf-41c2-be41-42e78ef95c0e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "{ご自身のスキーマ名を入力}",
      "label": "スキーマ",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "{ご自身のスキーマ名を入力}",
      "label": "スキーマ",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "suffix": {
     "currentValue": "twatanabe",
     "nuid": "745a9ed8-3c79-45f6-88ac-f9e5e10c25c9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "{ご自身のSuffixを指定}",
      "label": "ENDPOINT用の接尾辞",
      "name": "suffix",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "{ご自身のSuffixを指定}",
      "label": "ENDPOINT用の接尾辞",
      "name": "suffix",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
