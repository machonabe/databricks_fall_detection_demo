{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad70d36-ccda-4476-a1c9-baf238ac9735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------#\n",
    "# 1. ライブラリの準備\n",
    "# Databricks Runtime 14.x 以降推奨\n",
    "%pip install --upgrade databricks-langchain databricks-vectorsearch mlflow langgraph pillow sentence-transformers\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37e781f-216d-4774-8196-e3f4aa4bbf24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "import os\n",
    "import tempfile\n",
    "import mlflow\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "import uuid\n",
    "from typing import List, Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "# 必要なライブラリのインポート\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    VectorSearchRetrieverTool,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "# sentence-transformers のインポート（警告を抑制）\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_fast.*\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "print(\"✅ すべてのライブラリが正常にインポートされました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d7c75e2-f547-4c9d-84a1-6e1c0b2bd80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# 定数定義\n",
    "\n",
    "# Widgetsの作成\n",
    "dbutils.widgets.text(\"catalog\", \"fall_detection_demo_catalog\", \"カタログ\")\n",
    "dbutils.widgets.text(\"schema\", \"{ご自身のスキーマ名を入力}\", \"スキーマ\")\n",
    "dbutils.widgets.text(\"suffix\", \"{ご自身のSuffixを指定}\", \"ENDPOINT用の接尾辞\")\n",
    "dbutils.widgets.dropdown(\"recreate_schema\", \"False\", [\"True\", \"False\"], \"スキーマを再作成\")\n",
    "\n",
    "\n",
    "# Widgetからの値の取得\n",
    "CATALOG = dbutils.widgets.get(\"catalog\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema\")\n",
    "RECREATE_SCHEMA = dbutils.widgets.get(\"recreate_schema\") == \"True\"\n",
    "SUFFIX = dbutils.widgets.get(\"suffix\")\n",
    "\n",
    "# Volume パス定義\n",
    "VIDEO_VOL = f\"/Volumes/{CATALOG}/{SCHEMA}/video_volume\"\n",
    "FRAME_VOL = f\"/Volumes/{CATALOG}/{SCHEMA}/frame_volume\"\n",
    "\n",
    "INDEX_NAME = f\"{CATALOG}.{SCHEMA}.fall_detection_index\"\n",
    "\n",
    "VS_ENDPOINT = f\"fall_detection_vector_search_{SUFFIX}\"\n",
    "LLM_ENDPOINT = \"databricks-claude-3-7-sonnet\"\n",
    "\n",
    "\n",
    "# MLflow 自動ログ有効化\n",
    "mlflow.langchain.autolog()\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "# CLIP モデルの初期化\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    clip_model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "print(\"✅ CLIP モデルが正常に初期化されました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c71542e-2bf1-4327-b846-766ddff64766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------#\n",
    "mlflow.langchain.autolog()\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ff41b8-5b8d-413c-a067-2de7a33247f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# 修正版: Embeddings 基底クラスを継承したCLIPエンベディング\n",
    "class CLIPTextEmbeddings(Embeddings):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return [self.model.encode(text).tolist() for text in texts]\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.model.encode(text).tolist()\n",
    "\n",
    "clip_embeddings = CLIPTextEmbeddings(clip_model)\n",
    "\n",
    "# VectorSearchRetrieverTool の作成\n",
    "vs_tool = VectorSearchRetrieverTool(\n",
    "    index_name        = INDEX_NAME,\n",
    "    endpoint_name     = VS_ENDPOINT,\n",
    "    embedding         = clip_embeddings,\n",
    "    text_column       = \"image_path\",  # 必須パラメータ\n",
    "    columns           = [\"image_path\"],\n",
    "    num_results       = 5,\n",
    "    tool_name         = f\"fall_detection_image_search_{SUFFIX}\",\n",
    "    tool_description  = (\n",
    "        \"防犯カメラのフレーム画像を自然言語クエリで検索し、\"\n",
    "        \"類似度の高い画像パスを返します。\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"✅ VectorSearchRetrieverTool が正常に作成されました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6204997e-ba8d-4f5e-91ce-4425d3d607c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# 修正版: text_column パラメータを追加\n",
    "try:\n",
    "    vs_tool = VectorSearchRetrieverTool(\n",
    "        index_name        = INDEX_NAME,\n",
    "        endpoint_name     = VS_ENDPOINT,\n",
    "        embedding         = clip_embeddings,\n",
    "        text_column       = \"image_path\",          # ← 追加: 検索対象のテキストカラム\n",
    "        columns           = [\"image_path\"],\n",
    "        num_results       = 5,\n",
    "        tool_name         = \"fall_detection_image_search\",\n",
    "        tool_description  = (\n",
    "            \"防犯カメラのフレーム画像を自然言語クエリで検索し、\"\n",
    "            \"類似度の高い画像パスを返します。\"\n",
    "        ),\n",
    "    )\n",
    "    print(\"✅ VectorSearchRetrieverTool が正常に作成されました\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ エラーが発生しました: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd894fc-5559-4016-bdf0-4bcf34ffb630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# 完全修正版: OpenAI形式メッセージ変換対応エージェント\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    \"\"\"OpenAI形式メッセージ変換に対応した LangGraph ChatAgent\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_graph):\n",
    "        self.agent_graph = agent_graph\n",
    "\n",
    "    def _generate_unique_id(self) -> str:\n",
    "        \"\"\"一意な ID を生成\"\"\"\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "    def _convert_to_openai_format(self, messages: List[ChatAgentMessage]) -> List[dict]:\n",
    "        \"\"\"ChatAgentMessage を OpenAI 形式の辞書に変換\"\"\"\n",
    "        openai_messages = []\n",
    "        \n",
    "        for msg in messages:\n",
    "            # OpenAI 形式: {'role': 'user/assistant/system', 'content': '...'}\n",
    "            openai_msg = {\n",
    "                \"role\": msg.role,\n",
    "                \"content\": msg.content or \"\",\n",
    "            }\n",
    "            \n",
    "            # オプションフィールドの追加\n",
    "            if msg.name:\n",
    "                openai_msg[\"name\"] = msg.name\n",
    "            if msg.tool_calls:\n",
    "                openai_msg[\"tool_calls\"] = msg.tool_calls\n",
    "            if msg.tool_call_id:\n",
    "                openai_msg[\"tool_call_id\"] = msg.tool_call_id\n",
    "                \n",
    "            openai_messages.append(openai_msg)\n",
    "        \n",
    "        return openai_messages\n",
    "\n",
    "    def _convert_from_langchain_message(self, lc_msg_dict: dict) -> ChatAgentMessage:\n",
    "        \"\"\"LangChain メッセージ辞書を ChatAgentMessage に変換\"\"\"\n",
    "        \n",
    "        # LangChain の message_to_dict 形式: {'type': 'human', 'data': {...}}\n",
    "        if 'type' in lc_msg_dict and 'data' in lc_msg_dict:\n",
    "            msg_type = lc_msg_dict['type']\n",
    "            msg_data = lc_msg_dict['data']\n",
    "            \n",
    "            # type を role にマッピング\n",
    "            role_mapping = {\n",
    "                'human': 'user',\n",
    "                'ai': 'assistant', \n",
    "                'system': 'system',\n",
    "                'tool': 'tool'\n",
    "            }\n",
    "            \n",
    "            role = role_mapping.get(msg_type, 'assistant')\n",
    "            content = msg_data.get('content', '')\n",
    "            \n",
    "            return ChatAgentMessage(\n",
    "                id=msg_data.get('id') or self._generate_unique_id(),\n",
    "                role=role,\n",
    "                content=content,\n",
    "                name=msg_data.get('name'),\n",
    "                tool_calls=msg_data.get('tool_calls'),\n",
    "                tool_call_id=msg_data.get('tool_call_id'),\n",
    "                attachments=msg_data.get('attachments')\n",
    "            )\n",
    "        \n",
    "        # 直接的な辞書形式の場合\n",
    "        else:\n",
    "            return ChatAgentMessage(\n",
    "                id=lc_msg_dict.get('id') or self._generate_unique_id(),\n",
    "                role=lc_msg_dict.get('role', 'assistant'),\n",
    "                content=lc_msg_dict.get('content', ''),\n",
    "                name=lc_msg_dict.get('name'),\n",
    "                tool_calls=lc_msg_dict.get('tool_calls'),\n",
    "                tool_call_id=lc_msg_dict.get('tool_call_id'),\n",
    "                attachments=lc_msg_dict.get('attachments')\n",
    "            )\n",
    "\n",
    "    def predict(self, messages: List[ChatAgentMessage], context: ChatContext = None, custom_inputs=None) -> ChatAgentResponse:\n",
    "        \"\"\"修正版 predict メソッド: OpenAI形式変換対応\"\"\"\n",
    "        try:\n",
    "            # ChatAgentMessage を OpenAI 形式に変換\n",
    "            openai_messages = self._convert_to_openai_format(messages)\n",
    "            request = {\"messages\": openai_messages}\n",
    "            \n",
    "            print(f\"🔄 OpenAI形式メッセージ: {json.dumps(openai_messages, ensure_ascii=False, indent=2)}\")\n",
    "            \n",
    "            all_messages = []\n",
    "            for event in self.agent_graph.stream(request, stream_mode=\"updates\"):\n",
    "                print(f\"📨 Event: {event}\")\n",
    "                \n",
    "                for node_name, node_data in event.items():\n",
    "                    if node_data and \"messages\" in node_data:\n",
    "                        for msg_data in node_data[\"messages\"]:\n",
    "                            if msg_data is not None:\n",
    "                                try:\n",
    "                                    # LangChain メッセージを ChatAgentMessage に変換\n",
    "                                    chat_msg = self._convert_from_langchain_message(msg_data)\n",
    "                                    all_messages.append(chat_msg)\n",
    "                                    print(f\"✅ 変換成功: {chat_msg.role} - {chat_msg.content[:100]}...\")\n",
    "                                except Exception as conv_error:\n",
    "                                    print(f\"⚠️ メッセージ変換エラー: {conv_error}\")\n",
    "                                    # フォールバック: 基本的なアシスタントメッセージを作成\n",
    "                                    fallback_msg = ChatAgentMessage(\n",
    "                                        id=self._generate_unique_id(),\n",
    "                                        role=\"assistant\",\n",
    "                                        content=str(msg_data)\n",
    "                                    )\n",
    "                                    all_messages.append(fallback_msg)\n",
    "            \n",
    "            if not all_messages:\n",
    "                # メッセージが一つもない場合のフォールバック\n",
    "                fallback_msg = ChatAgentMessage(\n",
    "                    id=self._generate_unique_id(),\n",
    "                    role=\"assistant\",\n",
    "                    content=\"画像検索が完了しましたが、結果の取得に問題が発生しました。\"\n",
    "                )\n",
    "                all_messages.append(fallback_msg)\n",
    "            \n",
    "            return ChatAgentResponse(messages=all_messages)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ エージェント実行エラー: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # エラー時のフォールバック応答（ID 付き）\n",
    "            error_message = ChatAgentMessage(\n",
    "                id=self._generate_unique_id(),\n",
    "                role=\"assistant\",\n",
    "                content=f\"画像検索中にエラーが発生しました: {str(e)}\"\n",
    "            )\n",
    "            return ChatAgentResponse(messages=[error_message])\n",
    "\n",
    "print(\"✅ LangGraphChatAgent クラスが作成されました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1118fda8-d5dc-4c27-b044-8a77a9ae67af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# エージェントの構築\n",
    "tools = [vs_tool]\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT)\n",
    "system_prompt = (\n",
    "    \"あなたは防犯カメラ画像検索のアシスタントです。\"\n",
    "    \"ユーザーが入力した自然言語を使って画像を検索し、\"\n",
    "    \"最も関連する画像パスを返してください。\"\n",
    ")\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Sequence[ToolNode],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    bound_model = model.bind_tools(tools)\n",
    "\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        last = state[\"messages\"][-1]\n",
    "        return \"continue\" if last.get(\"tool_calls\") else \"end\"\n",
    "\n",
    "    preproc = (\n",
    "        RunnableLambda(lambda s: [{\"role\": \"system\", \"content\": system_prompt}] + s[\"messages\"])\n",
    "        if system_prompt else\n",
    "        RunnableLambda(lambda s: s[\"messages\"])\n",
    "    )\n",
    "    model_runnable = preproc | bound_model\n",
    "\n",
    "    def call_model(state: ChatAgentState, config: RunnableConfig):\n",
    "        resp = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [resp]}\n",
    "\n",
    "    sg = StateGraph(ChatAgentState)\n",
    "    sg.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    sg.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "    sg.set_entry_point(\"agent\")\n",
    "    sg.add_conditional_edges(\"agent\", should_continue, {\"continue\": \"tools\", \"end\": END})\n",
    "    sg.add_edge(\"tools\", \"agent\")\n",
    "    return sg.compile()\n",
    "\n",
    "# エージェントの作成\n",
    "agent_graph = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent_graph)\n",
    "\n",
    "# MLflow モデルとしてログ\n",
    "mlflow.models.set_model(AGENT)\n",
    "\n",
    "print(\"✅ エージェントが正常に作成されました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7934581-303e-4190-9d42-be0e2c51fddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# 修正版テストコード（画像表示機能付き）\n",
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "query_text = \"person lying on the floor\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"image_search_with_display\") as run:\n",
    "    try:\n",
    "        # ユーザーメッセージの作成（ID 付き）\n",
    "        user_msg = ChatAgentMessage(\n",
    "            id=str(uuid.uuid4()),\n",
    "            role=\"user\",\n",
    "            content=query_text\n",
    "        )\n",
    "        print(f\"✅ ユーザーメッセージ作成: ID={user_msg.id}\")\n",
    "        \n",
    "        # エージェント呼び出し\n",
    "        print(\"🔄 エージェント呼び出し開始...\")\n",
    "        response = AGENT.predict(messages=[user_msg])\n",
    "        \n",
    "        if response and response.messages:\n",
    "            print(f\"✅ 応答メッセージ数: {len(response.messages)}\")\n",
    "            \n",
    "            # 検索結果から画像パスを抽出\n",
    "            image_paths = []\n",
    "            \n",
    "            for i, msg in enumerate(response.messages):\n",
    "                print(f\"📋 メッセージ{i+1}: ID={msg.id}, Role={msg.role}\")\n",
    "                print(f\"   内容: {msg.content[:200]}...\")\n",
    "                \n",
    "                # メッセージ内容から画像パスを抽出\n",
    "                if msg.role == \"assistant\" and msg.content:\n",
    "                    # JSONフォーマットの検索結果を解析\n",
    "                    try:\n",
    "                        # JSONとして解析を試行\n",
    "                        if msg.content.strip().startswith('[') or msg.content.strip().startswith('{'):\n",
    "                            parsed_content = json.loads(msg.content)\n",
    "                            if isinstance(parsed_content, list):\n",
    "                                for item in parsed_content:\n",
    "                                    if isinstance(item, dict) and 'image_path' in item:\n",
    "                                        image_paths.append(item['image_path'])\n",
    "                            elif isinstance(parsed_content, dict) and 'image_path' in parsed_content:\n",
    "                                image_paths.append(parsed_content['image_path'])\n",
    "                    except json.JSONDecodeError:\n",
    "                        # JSON解析に失敗した場合、正規表現でパスを抽出\n",
    "                        pass\n",
    "                    \n",
    "                    # 正規表現でファイルパスを抽出（/で始まり.jpgで終わるパターン）\n",
    "                    path_pattern = r'/[^\\s]*\\.(?:jpg|jpeg|png|gif|bmp)'\n",
    "                    found_paths = re.findall(path_pattern, msg.content, re.IGNORECASE)\n",
    "                    image_paths.extend(found_paths)\n",
    "                    \n",
    "                    # フレームボリューム内のパスパターンも検索\n",
    "                    frame_vol_pattern = r'/Volumes/[^\\s]*\\.(?:jpg|jpeg|png|gif|bmp)'\n",
    "                    frame_paths = re.findall(frame_vol_pattern, msg.content, re.IGNORECASE)\n",
    "                    image_paths.extend(frame_paths)\n",
    "            \n",
    "            # 重複を除去\n",
    "            image_paths = list(set(image_paths))\n",
    "            \n",
    "            assistant = response.messages[-1]\n",
    "            \n",
    "            # MLflow ログ\n",
    "            mlflow.log_param(\"query_text\", query_text)\n",
    "            mlflow.log_param(\"response_status\", \"success\")\n",
    "            mlflow.log_param(\"agent_response\", assistant.content)\n",
    "            mlflow.log_param(\"message_count\", len(response.messages))\n",
    "            mlflow.log_param(\"embedding_model\", \"clip-ViT-B-32\")\n",
    "            mlflow.log_param(\"found_image_count\", len(image_paths))\n",
    "            mlflow.log_param(\"image_paths\", \";\".join(image_paths))\n",
    "            \n",
    "            # 検索結果の画像をすべて表示\n",
    "            print(f\"\\n🖼️ 検索された画像数: {len(image_paths)}\")\n",
    "            \n",
    "            if image_paths:\n",
    "                for idx, img_path in enumerate(image_paths, 1):\n",
    "                    try:\n",
    "                        if os.path.exists(img_path):\n",
    "                            print(f\"\\n📷 画像 {idx}: {img_path}\")\n",
    "                            \n",
    "                            # 画像をロードして表示\n",
    "                            img = Image.open(img_path)\n",
    "                            \n",
    "                            # 画像のサイズ情報を表示\n",
    "                            print(f\"   サイズ: {img.size[0]} x {img.size[1]} ピクセル\")\n",
    "                            \n",
    "                            # Databricks ノートブックに画像を表示\n",
    "                            display(img)\n",
    "                            \n",
    "                            # サムネイル作成（MLflow用）\n",
    "                            thumbnail = img.copy()\n",
    "                            thumbnail.thumbnail((300, 300))  # 300x300のサムネイル\n",
    "                            \n",
    "                            # 一時ディレクトリにサムネイルを保存\n",
    "                            thumb_filename = f\"thumbnail_{idx}_{os.path.basename(img_path)}\"\n",
    "                            thumb_path = os.path.join(\"/tmp\", thumb_filename)\n",
    "                            thumbnail.save(thumb_path)\n",
    "                            \n",
    "                            # MLflowにサムネイルをログ\n",
    "                            mlflow.log_artifact(thumb_path, artifact_path=\"search_result_thumbnails\")\n",
    "                            \n",
    "                        else:\n",
    "                            print(f\"⚠️ 画像ファイルが見つかりません: {img_path}\")\n",
    "                            \n",
    "                    except Exception as img_error:\n",
    "                        print(f\"❌ 画像表示エラー ({img_path}): {str(img_error)}\")\n",
    "                \n",
    "                # 画像一覧をMarkdown形式でも表示\n",
    "                print(\"\\n## 検索結果画像一覧\")\n",
    "                for idx, img_path in enumerate(image_paths, 1):\n",
    "                    print(f\"{idx}. `{img_path}`\")\n",
    "                    \n",
    "            else:\n",
    "                print(\"❌ 検索結果に画像パスが見つかりませんでした\")\n",
    "                print(\"デバッグ用: エージェントの応答内容:\")\n",
    "                for msg in response.messages:\n",
    "                    print(f\"  - {msg.role}: {msg.content}\")\n",
    "            \n",
    "            print(f\"\\n✅ MLflow run: {mlflow.get_artifact_uri()}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ エージェントからの応答がありません\")\n",
    "            mlflow.log_param(\"response_status\", \"no_response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ エラー発生: {str(e)}\")\n",
    "        mlflow.log_param(\"error_message\", str(e))\n",
    "        mlflow.log_param(\"response_status\", \"error\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "create_agent_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
